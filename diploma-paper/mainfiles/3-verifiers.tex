\section{Анализ алгоритмов и верификаторов}

\subsection{Alpha-Beta-CROWN}

Alpha-Beta-CROWN представляет собой вершину эволюции методов распространения границ (Bound Propagation). 
Его успех базируется на двух ключевых компонентах, обозначенных греческими буквами в названии. \\
$\alpha$-CROWN (Alpha): Оптимизация наклонов \\
Базовый алгоритм CROWN (эквивалентный DeepPoly) использует линейные релаксации для функций активации. 
Для ReLU $y = \max(0, x)$ при $x \in [l, u]$ (где $l < 0 < u$) нижняя граница описывается 
прямой линией $\lambda x$, где $0 \le \lambda \le 1$. Т
радиционные методы фиксируют $\lambda$ эвристически (например, $\lambda = u/(u-l)$). \\
$\alpha$-CROWN трактует $\lambda$ как оптимизируемый параметр $\alpha$. 
Алгоритм запускает градиентный спуск (аналогичный обучению сети) для поиска таких значений 
$\alpha$ для каждого нейрона, которые максимизируют нижнюю границу выхода сети (или минимизируют верхнюю). 
Это позволяет получать максимально точные (tight) границы без ветвления, 
что часто достаточно для верификации "легких" свойств. \cite{AlphaBetaCrownVNNCOMP23}
$\beta$-CROWN (Beta): Полная верификация без LP \\
Когда $\alpha$-CROWN недостаточно, требуется ветвление (Branch-and-Bound). Традиционные методы (как MIPVerify) 
решают в каждом листе дерева поиска задачу линейного программирования (LP), что медленно и требует переноса данных с GPU на CPU.
$\beta$-CROWN решает эту проблему, кодируя ограничения ветвления ("нейрон $i$ > 0" или "нейрон $i$ <= 0") 
непосредственно в процедуру распространения границ через двойственные переменные Лагранжа ($\beta$). Это позволяет:
\begin{itemize}
    \item Избежать вызова внешних LP-решателей.
    \item Выполнять оценку границ для тысяч веток параллельно на GPU как серию матричных умножений.
    \item Оптимизировать параметры ветвления $\beta$ совместно с $\alpha$ через градиентный спуск. \cite{AlphaBetaCrownVNNCOMP23}
\end{itemize}

\subsection{VeriNet и nnenum}
VeriNet фокусируется на методе Symbolic Interval Propagation (SIP). 
В отличие от CROWN, который распространяет линейные неравенства, SIP сохраняет зависимости в виде символьных выражений. 
VeriNet особенно эффективен в стратегии Input Splitting (расщепление входного пространства), 
что делает его предпочтительным выбором для задач с малой размерностью входа (робототехника, контроллеры ACAS Xu), 
где можно покрыть входное пространство сеткой. 
Алгоритм также использует итеративное уточнение границ, минимизируя накопление ошибки овер-аппроксимации.\cite{IntervalReachabilityAnalysis} \\

nnenum (победитель VNN-COMP 2020) использует стратегию Abstraction Refinement. 
Инструмент динамически переключается между различными представлениями множеств:
\begin{itemize}
    \item Zonotopes: Быстрые, но грубые.
    \item Star Sets (ImageStars): Точные, но вычислительно дорогие.
    \item Polyhedra: Используются при необходимости максимальной точности.
\end{itemize}
nnenum был первым инструментом, который показал, что умное управление уровнем абстракции может быть эффективнее, чем использование одного (пусть и мощного) метода для всей сети.\cite{NNEnumPaper}


\subsection{Marabou 2.0 и NeuralSAT}

Marabou 2.0 представляет собой современное развитие SMT-подхода. 
Ключевым нововведением стала замена классического поиска на процедуру DeepSoI (Sum of Infeasibilities). 
Вместо бинарного поиска выполняющего присваивания, DeepSoI минимизирует непрерывную функцию, 
представляющую "суммарное нарушение" ограничений ReLU. Это позволяет алгоритму "скользить" к решению, 
используя градиентную информацию, что значительно быстрее чистого комбинаторного перебора.\cite{arXiv2401Marabou2} \\

NeuralSAT переносит в нейросетевую верификацию мощь современных SAT-решателей, использующих CDCL (Conflict-Driven Clause Learning). 
В процессе поиска NeuralSAT выявляет несовместные комбинации состояний нейронов 
(например, "если нейрон A активен, то нейрон B не может быть активен") и добавляет их в базу знаний 
как "конфликтные клаузы" (lemmas). Это предотвращает повторный просмотр заведомо тупиковых ветвей поиска в будущем, 
обеспечивая прунинг (отсечение) огромных поддеревьев. 
Этот подход особенно эффективен для сетей со сложной внутренней логической структурой.\cite{NeuralSATRepo}


\subsection{GPU-ускорение и библиотека \texorpdfstring{\texttt{auto\_LiRPA}}{auto LiRPA}}
Современные достижения в верификации были бы невозможны без глубокой оптимизации алгоритмов под аппаратное обеспечение.
\subsubsection{GPU-ускорение и библиотека \texorpdfstring{\texttt{auto\_LiRPA}}{auto LiRPA}}
Традиционные алгоритмы верификации (Simplex, SMT) по своей природе последовательны и требуют сложного 
управления потоком управления (branching logic), что 
плохо подходит для архитектуры GPU (Single Instruction, Multiple Data - SIMD).
Прорыв произошел с созданием библиотеки \texttt{auto\_LiRPA}.\cite{AlphaBetaCrownVNNCOMP23}

\begin{itemize}
    \item \textbf{Идея:} Представить алгоритмы распространения границ (Bound Propagation) как операции над тензорами в вычислительном графе (аналогично тому, как PyTorch выполняет forward/backward pass).
    \item \textbf{Реализация:} Библиотека автоматически строит граф вычислений для верхней и нижней границ любого узла сети. Это позволяет верифицировать любую архитектуру, поддерживаемую PyTorch (CNN, ResNet, Transformer, LSTM), без написания специализированного кода.
    \item \textbf{Batch BaB:} В методе Branch-and-Bound вместо последовательной обработки веток верификатор собирает пакет (batch) из сотен тысяч нерешённых подзадач. Границы для всех этих подзадач вычисляются одновременно за один проход GPU, что амортизирует накладные расходы на запуск ядер CUDA и обеспечивает утилизацию GPU, близкую к 100\%.\cite{LiRPAVerifyRepo}
\end{itemize}

\subsubsection{Метод секущих плоскостей (Cutting Planes)}
Для усиления линейных релаксаций без полного ветвления применяются методы генерации отсечений (Cutting Planes), заимствованные из целочисленного программирования.
\begin{itemize}
    \item \textbf{GCP-CROWN (General Cutting Planes):} Метод добавляет к задаче линейные неравенства, связывающие значения нескольких нейронов (например, входов и выходов слоя). Эти неравенства «отсекают» дробные решения, допустимые в LP-релаксации, но недопустимые для реальной ReLU-сети.
    \item \textbf{BICCOS (Branch-and-bound Inferred Cuts with COnstraint Strengthening):} Специализированная техника для Alpha-Beta-CROWN, которая генерирует отсечения на основе анализа логических импликаций в дереве поиска и применяет их итеративно. Это позволяет решать задачи, которые ранее требовали бы слишком глубокого дерева ветвления, за счёт усиления границ в корневых узлах.\cite{AutoLiRPAPracticeSlides}
\end{itemize}
